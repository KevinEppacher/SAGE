import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from tf2_msgs.msg import TFMessage
from cv_bridge import CvBridge
import cv2
from openfusion_ros.slam import build_slam, BaseSLAM
import numpy as np
from argparse import Namespace
import open3d as o3d
from scipy.spatial.transform import Rotation as R
from geometry_msgs.msg import TransformStamped
from tf2_msgs.msg import TFMessage
from tf_transformations import quaternion_matrix
import tf_transformations
from tf2_ros import Buffer, TransformListener
from openfusion_ros.utils import (
    show_pc, save_pc, get_cmap_legend
)
from sensor_msgs.msg import PointCloud2, PointField
from std_msgs.msg import Header
import sensor_msgs_py.point_cloud2 as pc2
from rosgraph_msgs.msg import Clock
from builtin_interfaces.msg import Time
from visualization_msgs.msg import Marker
# Import PoseArray
from geometry_msgs.msg import Twist, PoseWithCovarianceStamped, PoseArray, Pose
from openfusion_ros.configs.build import get_config
from openfusion_ros.datasets import Dataset
from sensor_msgs.msg import CameraInfo
from std_msgs.msg import String
from rcl_interfaces.msg import SetParametersResult

# ANSI color codes
BLUE = "\033[94m"
GREEN = "\033[92m"
YELLOW = "\033[93m"
RED = "\033[91m"
BOLD = "\033[1m"
RESET = "\033[0m"

def transform_to_matrix(transform_stamped):
    t = transform_stamped.transform.translation
    q = transform_stamped.transform.rotation
    matrix = quaternion_matrix([q.x, q.y, q.z, q.w])
    matrix[0, 3] = t.x
    matrix[1, 3] = t.y
    matrix[2, 3] = t.z
    return matrix

def time_to_float(t: Time):
    return t.sec + t.nanosec * 1e-9

def convert_stamp_to_sec(stamp):
    return stamp.sec + stamp.nanosec * 1e-9
    
def pose_msg_to_matrix(pose_msg: Pose):
    q = [pose_msg.orientation.x, pose_msg.orientation.y,
         pose_msg.orientation.z, pose_msg.orientation.w]
    T = quaternion_matrix(q)
    T[0, 3] = pose_msg.position.x
    T[1, 3] = pose_msg.position.y
    T[2, 3] = pose_msg.position.z
    return T

def scale_intrinsics(K, old_size, new_size):
    scale_x = new_size[0] / old_size[0]
    scale_y = new_size[1] / old_size[1]

    K_scaled = K.copy()
    K_scaled[0, 0] *= scale_x  # fx
    K_scaled[1, 1] *= scale_y  # fy
    K_scaled[0, 2] *= scale_x  # cx
    K_scaled[1, 2] *= scale_y  # cy
    return K_scaled

class SemanticQuery:
    def __init__(self, default_query="none"):
        self.text_query = default_query
        self.previous_text_query = default_query
        self.image_query = None
        self.previous_image_query = None

class OpenFusionNode(Node):
    def __init__(self):
        super().__init__('openfusion_node')
        # Declare and get parameters
        self.declare_parameter("append_poses_frequency", 10.0)
        self.declare_parameter("pointcloud_frequency", 1.0)
        self.declare_parameter("pose_min_translation", 0.05)
        self.declare_parameter("pose_min_rotation", 5.0)
        self.declare_parameter("parent_frame", "map")
        self.declare_parameter("child_frame", "camera")
        self.declare_parameter("topk", 10)
        self.declare_parameter("depth_max", 10.0)
        self.declare_parameter("logging.enabled", False)
        self.declare_parameter("logging.log_file", "openfusion.log")

        # Assign to class variables
        self.append_poses_frequency = self.get_parameter("append_poses_frequency").get_parameter_value().double_value
        self.pcl_frequency = self.get_parameter("pointcloud_frequency").get_parameter_value().double_value
        self.pose_min_translation = self.get_parameter("pose_min_translation").get_parameter_value().double_value
        self.pose_min_rotation = self.get_parameter("pose_min_rotation").get_parameter_value().double_value
        self.parent_frame = self.get_parameter("parent_frame").get_parameter_value().string_value
        self.child_frame = self.get_parameter("child_frame").get_parameter_value().string_value
        self.topk = self.get_parameter("topk").get_parameter_value().integer_value
        self.depth_max = self.get_parameter("depth_max").get_parameter_value().double_value
        self.logging = self.get_parameter("logging.enabled").get_parameter_value().bool_value
        self.log_file = self.get_parameter("logging.log_file").get_parameter_value().string_value

        # rgb, depth, pose, and their timestamps
        self.rgb = None
        self.depth = None
        self.pose = None
        self.rgb_stamp = None
        self.depth_stamp = None
        self.pose_stamp = None
        # Latest clock for TF buffer
        self.latest_clock = None
        # Buffer for storing poses, rgb and depth images
        self.pose_buffer = []
        self.rgb_buffer = []
        self.depth_buffer = []
        # OpenCV bridge for converting ROS images to OpenCV format
        self.bridge = CvBridge()
        # Print options for numpy for better readability
        np.set_printoptions(precision=3, suppress=True)
        # Declare OpenFusionNode State Machine
        self.state = "building"
        self.semantic_input = SemanticQuery()
        self.camera_initialized = False

        # Publishers and subscribers
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        # Publishers
        self.pc_pub = self.create_publisher(PointCloud2, 'pointcloud', 10)
        self.pose_pub = self.create_publisher(PoseArray, 'pose_array', 10)
        self.semantic_pc_pub = self.create_publisher(PointCloud2,'semantic_pointcloud',10)
        self.depth_pc_pub = self.create_publisher(PointCloud2, 'depth_pointcloud', 10)
        # Subscribers
        self.rgb_sub = self.create_subscription(Image, '/rgb', self.rgb_callback, 10)
        self.depth_sub = self.create_subscription(Image, '/depth', self.depth_callback, 10)
        self.clock_sub = self.create_subscription(Clock, '/clock', self.clock_callback, 10)
        self.query_sub = self.create_subscription(String,'/semantic/text_query',self.text_query_callback,10)
        self.camera_info_sub = self.create_subscription(CameraInfo,'/camera_info',self.camera_info_callback,10)
        # Add dynamic reconfigure
        self.add_on_set_parameters_callback(self.parameter_update_callback)

        self.timers_started = False

        self.print_all_parameters()
        self.get_logger().info("OpenFusionNode initialized")

    def rgb_callback(self, msg):
        try:
            self.rgb = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            self.rgb_stamp = msg.header.stamp
        except Exception as e:
            self.get_logger().error(f"Failed to convert RGB image: {e}")

    def depth_callback(self, msg):
        try:
            depth_img = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')
            if depth_img.dtype == np.float32:
                depth_img = (depth_img * 1000.0).astype(np.uint16)
            elif depth_img.dtype != np.uint16:
                self.get_logger().warn(f"Unexpected depth dtype: {depth_img.dtype}")
                return
            self.depth = np.ascontiguousarray(depth_img.astype(np.uint16))
            # self.depth = depth_img
            self.depth_stamp = msg.header.stamp
        except Exception as e:
            self.get_logger().error(f"Failed to convert depth image: {e}")

    def text_query_callback(self, msg: String):
        if (msg.data == self.semantic_input.previous_text_query):
            return
        
        self.semantic_input.text_query = msg.data

        self.get_logger().info(f"Updated semantic query to: {self.semantic_input.text_query}")
        self.semantic_input.previous_text_query = msg.data

    def camera_info_callback(self, msg: CameraInfo):
        if self.camera_initialized:
            return

        # Intrinsic matrix from CameraInfo (K is a 3x3 row-major list)
        K = np.array(msg.k, dtype=np.float64).reshape(3, 3)
        img_width = msg.width
        img_height = msg.height

        self.get_logger().info(f"Received camera info with resolution: {img_width}x{img_height}")

        params = {
            'path': '/app/src/OpenFusion/sample/scannet/scene0010_01',
            'depth_scale': 1000.0,
            'depth_max': self.depth_max,
            'voxel_size': 0.01953125,
            'block_resolution': 8,
            'block_count': 20000,
            'img_size': (img_width, img_height),
            'input_size': (img_width, img_height)  # Keep same if no resizing
        }

        intrinsic = scale_intrinsics(K, params['img_size'], params['input_size'])

        self.get_logger().info(f"Intrinsic matrix: \n{intrinsic}")

        args = Namespace(
            algo='vlfusion',
            vl='seem',
            data='scannet',
            scene='scene0010_01',
            frames=-1,
            device='cuda:0',
            live=False,
            stream=False,
            save=False,
            load=False,
            host_ip='YOUR IP'
        )

        self.get_logger().info("Building OpenFusion SLAM...")
        self.slam = build_slam(args, intrinsic, params)
        self.camera_initialized = True
        self.get_logger().info("OpenFusion SLAM initialized.")
        self.start_timers()

    def parameter_update_callback(self, params):
        for param in params:
            if param.name == "topk" and isinstance(param.value, int):
                self.topk = param.value
                self.get_logger().info(f"Dynamically updated topk to {self.topk}")
        return SetParametersResult(successful=True)

    def print_all_parameters(self):
        self.get_logger().info("OpenFusionNode parameters:")
        for name in [
            "append_poses_frequency",
            "pointcloud_frequency",
            "pose_min_translation",
            "pose_min_rotation",
            "parent_frame",
            "child_frame",
            "topk",
            "depth_max",
            "logging.enabled",
            "logging.log_file"
        ]:
            value = self.get_parameter(name).value
            self.get_logger().info(f"  {name}: {value}")

    def get_pose(self):
        try:
            
            transform = self.tf_buffer.lookup_transform(self.parent_frame, self.child_frame, self.get_timestamp())
            
            self.pose_stamp = transform.header.stamp            
            
            return transform_to_matrix(transform)
        
        except Exception as e:
            if self.logging:
                self.get_logger().warn(f"Transform not available: {e}")
            return None

    def get_rgb(self):
        self.rgb_stamp = self.get_timestamp()
        return self.rgb
    
    def get_depth(self):
        self.depth_stamp = self.get_timestamp()
        return self.depth

    def get_timestamp(self):
        return self.latest_clock if self.latest_clock is not None else self.get_clock().now().to_msg()

    def clock_callback(self, msg: Clock):
        current_time = msg.clock
        if self.latest_clock:
            prev = time_to_float(self.latest_clock)
            curr = time_to_float(current_time)
            if curr < prev:
                self.get_logger().warn("Detected rosbag loop! Clearing TF buffer.")
                self.tf_buffer.clear()

        self.latest_clock = current_time

    def publish_pointcloud(self, points, colors):
        if points is None or len(points) == 0:
            self.get_logger().warn("No points to publish")
            return
        colors = np.clip(colors, 0, 1)
        colors_uint8 = (colors * 255).astype(np.uint8)
        rgb_uint32 = (colors_uint8[:, 0].astype(np.uint32) << 16 |
                      colors_uint8[:, 1].astype(np.uint32) << 8 |
                      colors_uint8[:, 2].astype(np.uint32))
        cloud = [(x, y, z, rgb) for (x, y, z), rgb in zip(points, rgb_uint32)]
        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)
        ]
        header = Header()
        header.stamp = self.get_timestamp()
        header.frame_id = self.parent_frame
        pc2_msg = pc2.create_cloud(header, fields, cloud)
        self.pc_pub.publish(pc2_msg)

    def publish_semantic_pointcloud(self, points, colors):
        if points is None or len(points) == 0:
            self.get_logger().warn("No semantic points to publish")
            return

        # # Farbfilter: Rot hervorhebungen (wie in SEEM üblich)
        # red_mask = (colors[:, 0] > 0.9) & (colors[:, 1] < 0.2) & (colors[:, 2] < 0.2)
        # points = points[red_mask]
        # colors = colors[red_mask]

        # if len(points) == 0:
        #     self.get_logger().warn("No highlighted (red) semantic points found.")
        #     return

        colors_uint8 = (np.clip(colors, 0, 1) * 255).astype(np.uint8)
        rgb_uint32 = (colors_uint8[:, 0].astype(np.uint32) << 16 |
                    colors_uint8[:, 1].astype(np.uint32) << 8 |
                    colors_uint8[:, 2].astype(np.uint32))

        cloud = [(x, y, z, rgb) for (x, y, z), rgb in zip(points, rgb_uint32)]

        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)
        ]

        header = Header()
        header.stamp = self.get_timestamp()
        header.frame_id = self.parent_frame

        pc2_msg = pc2.create_cloud(header, fields, cloud)
        self.semantic_pc_pub.publish(pc2_msg)

    def publish_pose_array(self):
        pose_array = PoseArray()
        pose_array.header.stamp = self.get_timestamp()
        pose_array.header.frame_id = self.parent_frame

        for matrix in self.slam.point_state.poses:
            inverted_matrix = np.linalg.inv(matrix)
            pose = Pose()
            pose.position.x = inverted_matrix[0, 3]
            pose.position.y = inverted_matrix[1, 3]
            pose.position.z = inverted_matrix[2, 3]
            q = tf_transformations.quaternion_from_matrix(inverted_matrix)
            pose.orientation.x = q[0]
            pose.orientation.y = q[1]
            pose.orientation.z = q[2]
            pose.orientation.w = q[3]
            pose_array.poses.append(pose)

        self.pose_pub.publish(pose_array)
        # self.get_logger().info(f"Published PoseArray with {len(pose_array.poses)} poses from slam.point_state.")

    def publish_depth_pointcloud(self, rgb, depth, frame_id='camera_link'):
        if depth is None or rgb is None:
            self.get_logger().warn("Missing RGB or depth image for depth cloud.")
            return

        height, width = depth.shape
        fx = 916.2491
        fy = 916.2491
        cx = 640.0
        cy = 360.0
        scale = 1000.0  # mm → m

        points = []
        for v in range(0, height, 4):
            for u in range(0, width, 4):
                z = depth[v, u] / scale
                if z == 0:
                    continue
                x = (u - cx) * z / fx
                y = (v - cy) * z / fy
                color = rgb[v, u]
                r, g, b = color
                rgb_value = (int(r) << 16) | (int(g) << 8) | int(b)
                points.append((x, y, z, rgb_value))


        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)
        ]

        header = Header()
        header.stamp = self.get_timestamp()
        header.frame_id = frame_id

        pc2_msg = pc2.create_cloud(header, fields, points)
        self.depth_pc_pub.publish(pc2_msg)

    def is_pose_significantly_different(self, new_pose, prev_pose, trans_diff_threshold=0.05, rot_diff_threshold=5.0):
        """
        Check if the new pose is significantly different from the previous pose.
        Args:
            new_pose (np.ndarray): The new pose matrix.
            prev_pose (np.ndarray): The previous pose matrix.
        """
        # Check if the new pose is significantly different from the previous pose
        if prev_pose is None:
            return True
        
        # Calclate translation diffrence
        trans_diff = np.linalg.norm(new_pose[:3, 3] - prev_pose[:3, 3])
        # Calculate rotation difference
        r1 = R.from_matrix(prev_pose[:3, :3])
        r2 = R.from_matrix(new_pose[:3, :3])
        delta_r = r1.inv() * r2
        angle_deg = np.degrees(np.abs(delta_r.magnitude()))

        return trans_diff > trans_diff_threshold or angle_deg > rot_diff_threshold

    def process_semantic_query(self):
        """Handles semantic query and publishing of the filtered pointcloud."""
        try:
            if isinstance(self.slam, BaseSLAM) and hasattr(self.slam, "query"):
                query_points, scores = self.slam.query(
                    self.semantic_input.text_query, topk=self.topk, only_poi=True
                )

                if query_points is not None and len(query_points) > 0:
                    query_colors = self.map_scores_to_colors(query_points, scores)
                    self.publish_semantic_pointcloud(query_points, query_colors)
                else:
                    self.get_logger().warn(f"Semantic query '{self.semantic_input.text_query}' returned no points.")
        except Exception as e:
            self.get_logger().error(f"Semantic query failed: {e}")

    def map_scores_to_colors(self, query_points, scores):
        """Converts semantic scores to red-scale RGB colors for visualization."""
        # Default minimum score (for points without explicit score)
        default_score = 0.2
        full_scores = np.full(query_points.shape[0], default_score, dtype=np.float32)

        # Fill known scores into full_scores array
        if scores is not None and len(scores) <= len(full_scores):
            full_scores[:len(scores)] = scores

        # Normalize scores and clamp to [0, 1]
        full_scores = np.nan_to_num(full_scores, nan=0.0, posinf=1.0, neginf=0.0)
        full_scores = np.clip(full_scores, 0.0, 1.0)
        full_scores = (full_scores - full_scores.min()) / (full_scores.max() - full_scores.min() + 1e-8)

        # Map to red gradient: dark red (low score) to bright red (high score)
        min_red = 0.4
        red_channel = min_red + full_scores * (1.0 - min_red)
        green_channel = np.zeros_like(red_channel)
        blue_channel = np.zeros_like(red_channel)

        return np.stack([red_channel, green_channel, blue_channel], axis=1)

    def apend_pose_timer_callback(self):
        if not self.camera_initialized:
            self.get_logger().warn("Camera not initialized yet. Skipping.")
            return

        # Get all the images and pose
        pose = self.get_pose()
        rgb = self.get_rgb()
        depth = self.get_depth()

        # Check if any of the images ore pose are None
        if pose is None or rgb is None or depth is None:
            reason = (
                "pose" if pose is None else
                "RGB" if rgb is None else
                "depth"
            )
            self.get_logger().warn(f"Skipping frame: {reason} is None.")
            return

        # Check if the pose is significantly different from the previous pose
        min_trans = self.pose_min_translation
        min_rot = self.pose_min_rotation 
        if self.pose_buffer:
            last_pose_matrix = pose_msg_to_matrix(self.pose_buffer[-1])
            if not self.is_pose_significantly_different(new_pose=pose, prev_pose=last_pose_matrix, 
                                                        trans_diff_threshold=min_trans, rot_diff_threshold=min_rot):
                self.get_logger().info("Pose not significantly different. Skipping frame.")
                return
        
        # Convert the timestamps to seconds
        rgb_time = convert_stamp_to_sec(self.rgb_stamp)
        depth_time = convert_stamp_to_sec(self.depth_stamp)
        pose_time = convert_stamp_to_sec(self.pose_stamp)

        # Check if the timestamps are synchronized
        max_diff = max(abs(rgb_time - depth_time), abs(rgb_time - pose_time), abs(depth_time - pose_time))
        if max_diff > 0.01:
            self.get_logger().warn(f"Timestamps not synchronized (Δ={max_diff:.3f}s). Skipping frame.")
            return
        
        # Transform the pose to the camera frame
        T_camera_map = np.linalg.inv(pose)

        # Update the new states for SLAM OpenFusion
        self.slam.io.update(rgb, depth, T_camera_map)
        self.slam.vo()
        self.slam.compute_state(encode_image=True)
        
    def pcl_timer_callback(self):
        if not self.camera_initialized:
            self.get_logger().warn("Camera not initialized yet. Skipping.")
            return

        # Step 1: Get input data
        rgb = self.get_rgb()
        depth = self.get_depth()

        # Step 2: Publish debug info
        self.publish_depth_pointcloud(rgb, depth, frame_id=self.child_frame)
        self.publish_pose_array()

        # Step 3: Run VO and fusion
        self.slam.vo()
        self.slam.compute_state(encode_image=True)

        # Step 4: Publish full scene point cloud
        points, colors = self.slam.point_state.get_pc()
        self.publish_pointcloud(points, colors)

        # Step 5: Semantic query
        if self.semantic_input.text_query == "none":
            self.get_logger().info("No semantic query provided. Skipping semantic pointcloud publishing.")
            return

        self.process_semantic_query()

    def start_timers(self):
        while not self.timers_started:
            if not self.append_poses_frequency > 0:
                self.get_logger().warn(f"append_poses_frequency is {self.append_poses_frequency}. Timer not started.")
                break
            if not self.pcl_frequency > 0:
                self.get_logger().warn(f"pointcloud_frequency is {self.pcl_frequency}. Timer not started.")
                break
            
            self.append_pose_timer = self.create_timer(1.0 / self.append_poses_frequency, self.apend_pose_timer_callback)
            self.publish_pcl_timer = self.create_timer(1.0 / self.pcl_frequency, self.pcl_timer_callback)
            self.timers_started = True
