import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image
from tf2_msgs.msg import TFMessage
from cv_bridge import CvBridge
import cv2
from openfusion_ros.slam import build_slam, BaseSLAM
import numpy as np
from argparse import Namespace
import open3d as o3d
from scipy.spatial.transform import Rotation as R
from geometry_msgs.msg import TransformStamped
from tf2_msgs.msg import TFMessage
from tf_transformations import quaternion_matrix
import tf_transformations
from tf2_ros import Buffer, TransformListener
from openfusion_ros.utils import (
    show_pc, save_pc, get_cmap_legend
)
from sensor_msgs.msg import PointCloud2, PointField
from std_msgs.msg import Header
import sensor_msgs_py.point_cloud2 as pc2
from rosgraph_msgs.msg import Clock
from builtin_interfaces.msg import Time
from visualization_msgs.msg import Marker
# Import PoseArray
from geometry_msgs.msg import Twist, PoseWithCovarianceStamped, PoseArray, Pose
from openfusion_ros.configs.build import get_config
from openfusion_ros.datasets import Dataset
from sensor_msgs.msg import CameraInfo
from std_msgs.msg import String

def transform_to_matrix(transform_stamped):
    t = transform_stamped.transform.translation
    q = transform_stamped.transform.rotation
    matrix = quaternion_matrix([q.x, q.y, q.z, q.w])
    matrix[0, 3] = t.x
    matrix[1, 3] = t.y
    matrix[2, 3] = t.z
    return matrix

def time_to_float(t: Time):
    return t.sec + t.nanosec * 1e-9

def convert_stamp_to_sec(stamp):
    return stamp.sec + stamp.nanosec * 1e-9
    
def pose_msg_to_matrix(pose_msg: Pose):
    q = [pose_msg.orientation.x, pose_msg.orientation.y,
         pose_msg.orientation.z, pose_msg.orientation.w]
    T = quaternion_matrix(q)
    T[0, 3] = pose_msg.position.x
    T[1, 3] = pose_msg.position.y
    T[2, 3] = pose_msg.position.z
    return T

def scale_intrinsics(K, old_size, new_size):
    scale_x = new_size[0] / old_size[0]
    scale_y = new_size[1] / old_size[1]

    K_scaled = K.copy()
    K_scaled[0, 0] *= scale_x  # fx
    K_scaled[1, 1] *= scale_y  # fy
    K_scaled[0, 2] *= scale_x  # cx
    K_scaled[1, 2] *= scale_y  # cy
    return K_scaled

class SemanticQuery:
    def __init__(self, default_query="none"):
        self.text_query = default_query
        self.image_query = None

class OpenFusionNode(Node):
    def __init__(self):
        super().__init__('openfusion_node')
        self.get_logger().info("OpenFusionNode initialized")

        # Initialize variables
        self.frequency = 10
        # rgb, depth, pose, and their timestamps
        self.rgb = None
        self.depth = None
        self.pose = None
        self.rgb_stamp = None
        self.depth_stamp = None
        self.pose_stamp = None
        # Latest clock for TF buffer
        self.latest_clock = None
        # Buffer for storing poses, rgb and depth images
        self.pose_buffer = []
        self.rgb_buffer = []
        self.depth_buffer = []
        # OpenCV bridge for converting ROS images to OpenCV format
        self.bridge = CvBridge()
        # Minimum translation and rotation for pose updates
        self.pose_min_translation = 0.05  # imeters
        self.pose_min_rotation = 5.0      # degrees
        # Print options for numpy for better readability
        np.set_printoptions(precision=3, suppress=True)
        # Declare OpenFusionNode State Machine
        self.state = "building"
        self.semantic_input = SemanticQuery()

        # Publishers and subscribers
        self.tf_buffer = Buffer()
        self.tf_listener = TransformListener(self.tf_buffer, self)
        # Publishers
        self.pc_pub = self.create_publisher(PointCloud2, '/openfusion/pointcloud', 10)
        self.pose_pub = self.create_publisher(PoseArray, '/openfusion/pose_array', 10)
        self.semantic_pc_pub = self.create_publisher(PointCloud2,'/openfusion/semantic_pointcloud',10)
        self.depth_pc_pub = self.create_publisher(PointCloud2, '/openfusion/depth_pointcloud', 10)
        # Subscribers
        self.rgb_sub = self.create_subscription(Image, '/rgb', self.rgb_callback, 10)
        self.depth_sub = self.create_subscription(Image, '/depth', self.depth_callback, 10)
        self.clock_sub = self.create_subscription(Clock, '/clock', self.clock_callback, 10)
        self.query_sub = self.create_subscription(String,'/semantic/text_query',self.text_query_callback,10)
        # Timers
        self.timer = self.create_timer(1/self.frequency, self.timer_callback)
        
        # Isaac Sim camera intrinsic parameters
        intrinsic_unscaled = np.array([
            [916.2491, 0.0, 640.0],
            [0.0, 916.2491, 360.0],
            [0.0, 0.0, 1.0]
        ], dtype=np.float64)

        # intrinsic = np.array([
        #     [288.9353025, 0.0,       159.75],
        #     [0.0,         288.9353025, 119.75],
        #     [0.0,         0.0,          1.0]
        # ], dtype=np.float64)

        params = {
            'path': '/app/src/OpenFusion/sample/scannet/scene0010_01',
            'depth_scale': 1000.0,
            'depth_max': 10.0,
            'voxel_size': 0.01953125,
            'block_resolution': 8,
            'block_count': 20000,
            # 'img_size': (640, 480),
            'img_size': (1280, 720),
            # 'input_size': (320, 240)
            'input_size': (1280, 720)
        }

        intrinsic = scale_intrinsics(intrinsic_unscaled, params['img_size'], params['input_size'])

        args = Namespace(
            algo='vlfusion',
            vl='seem',
            data='scannet',
            scene='scene0010_01',
            frames=-1,
            device='cuda:0',
            live=False,
            stream=False,
            save=False,
            load=False,
            host_ip='YOUR IP'
        )

        # params = get_config(args.data, args.scene)
        # self.dataset: Dataset = params["dataset"](params["path"], args.frames, args.stream)
        # intrinsic = self.dataset.load_intrinsics(params["img_size"], params["input_size"])

        self.slam = build_slam(args, intrinsic, params)

    def rgb_callback(self, msg):
        try:
            self.rgb = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            self.rgb_stamp = msg.header.stamp
        except Exception as e:
            self.get_logger().error(f"Failed to convert RGB image: {e}")

    def depth_callback(self, msg):
        try:
            depth_img = self.bridge.imgmsg_to_cv2(msg, desired_encoding='passthrough')
            if depth_img.dtype == np.float32:
                depth_img = (depth_img * 1000.0).astype(np.uint16)
            elif depth_img.dtype != np.uint16:
                self.get_logger().warn(f"Unexpected depth dtype: {depth_img.dtype}")
                return
            self.depth = np.ascontiguousarray(depth_img.astype(np.uint16))
            # self.depth = depth_img
            self.depth_stamp = msg.header.stamp
        except Exception as e:
            self.get_logger().error(f"Failed to convert depth image: {e}")

    def text_query_callback(self, msg: String):
        self.semantic_input.text_query = msg.data
        self.get_logger().info(f"Updated semantic query to: {self.semantic_input.text_query}")

    def get_pose(self):
        try:
            
            transform = self.tf_buffer.lookup_transform('map', 'camera', self.get_timestamp())
            
            self.pose_stamp = transform.header.stamp
            
            return transform_to_matrix(transform)
        
        except Exception as e:
        
            self.get_logger().warn(f"Transform not available: {e}")
        
            return None

    def get_rgb(self):
        self.rgb_stamp = self.get_timestamp()
        return self.rgb
    
    def get_depth(self):
        self.depth_stamp = self.get_timestamp()
        return self.depth

    def get_timestamp(self):
        return self.latest_clock if self.latest_clock is not None else self.get_clock().now().to_msg()

    def clock_callback(self, msg: Clock):
        current_time = msg.clock
        if self.latest_clock:
            prev = time_to_float(self.latest_clock)
            curr = time_to_float(current_time)
            if curr < prev:
                self.get_logger().warn("Detected rosbag loop! Clearing TF buffer.")
                self.tf_buffer.clear()

        self.latest_clock = current_time

    def publish_pointcloud(self, points, colors):
        if points is None or len(points) == 0:
            self.get_logger().warn("No points to publish")
            return
        colors = np.clip(colors, 0, 1)
        colors_uint8 = (colors * 255).astype(np.uint8)
        rgb_uint32 = (colors_uint8[:, 0].astype(np.uint32) << 16 |
                      colors_uint8[:, 1].astype(np.uint32) << 8 |
                      colors_uint8[:, 2].astype(np.uint32))
        cloud = [(x, y, z, rgb) for (x, y, z), rgb in zip(points, rgb_uint32)]
        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)
        ]
        header = Header()
        header.stamp = self.get_timestamp()
        header.frame_id = 'map'
        pc2_msg = pc2.create_cloud(header, fields, cloud)
        self.pc_pub.publish(pc2_msg)

    def publish_semantic_pointcloud(self, points, colors):
        if points is None or len(points) == 0:
            self.get_logger().warn("No semantic points to publish")
            return

        colors = np.clip(colors, 0, 1)
        colors_uint8 = (colors * 255).astype(np.uint8)
        rgb_uint32 = (colors_uint8[:, 0].astype(np.uint32) << 16 |
                    colors_uint8[:, 1].astype(np.uint32) << 8 |
                    colors_uint8[:, 2].astype(np.uint32))

        cloud = [(x, y, z, rgb) for (x, y, z), rgb in zip(points, rgb_uint32)]

        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)
        ]

        header = Header()
        header.stamp = self.get_timestamp()
        header.frame_id = 'map'

        pc2_msg = pc2.create_cloud(header, fields, cloud)
        self.semantic_pc_pub.publish(pc2_msg)

    def publish_pose_array(self):
        pose_array = PoseArray()
        pose_array.header.stamp = self.get_timestamp()
        pose_array.header.frame_id = 'map'

        for matrix in self.slam.point_state.poses:
            inverted_matrix = np.linalg.inv(matrix)
            pose = Pose()
            pose.position.x = inverted_matrix[0, 3]
            pose.position.y = inverted_matrix[1, 3]
            pose.position.z = inverted_matrix[2, 3]
            q = tf_transformations.quaternion_from_matrix(inverted_matrix)
            pose.orientation.x = q[0]
            pose.orientation.y = q[1]
            pose.orientation.z = q[2]
            pose.orientation.w = q[3]
            pose_array.poses.append(pose)

        self.pose_pub.publish(pose_array)
        self.get_logger().info(f"Published PoseArray with {len(pose_array.poses)} poses from slam.point_state.")

    def open_fusion_process(self, rgb, depth, pose):
        inverted_pose = np.linalg.inv(pose)
        self.slam.io.update(rgb, depth, inverted_pose)
        self.slam.vo()
        self.slam.compute_state(encode_image=True)
        points, colors = self.slam.point_state.get_pc()
        self.publish_pointcloud(points, colors)

        if self.semantic_input.text_query == "none":
            self.get_logger().info("No semantic query provided. Skipping semantic pointcloud publishing.")
            return

        try:
            if isinstance(self.slam, BaseSLAM) and hasattr(self.slam, "query"):
                points, colors = self.slam.query(self.semantic_input.text_query, topk=10)
                if points is not None and len(points) > 0:
                    self.publish_semantic_pointcloud(points, colors)
                else:
                    self.get_logger().warn(f"Semantic query '{self.semantic_input.text_query}' returned no points.")
        except Exception as e:
            self.get_logger().error(f"Semantic query failed: {e}")

    def publish_depth_pointcloud(self, rgb, depth, frame_id='camera'):
        if depth is None or rgb is None:
            self.get_logger().warn("Missing RGB or depth image for depth cloud.")
            return

        height, width = depth.shape
        fx = 916.2491
        fy = 916.2491
        cx = 640.0
        cy = 360.0
        scale = 1000.0  # mm → m

        points = []
        for v in range(0, height, 4):
            for u in range(0, width, 4):
                z = depth[v, u] / scale
                if z == 0:
                    continue
                x = (u - cx) * z / fx
                y = (v - cy) * z / fy
                color = rgb[v, u]
                r, g, b = color
                rgb_value = (int(r) << 16) | (int(g) << 8) | int(b)
                points.append((x, y, z, rgb_value))


        fields = [
            PointField(name='x', offset=0, datatype=PointField.FLOAT32, count=1),
            PointField(name='y', offset=4, datatype=PointField.FLOAT32, count=1),
            PointField(name='z', offset=8, datatype=PointField.FLOAT32, count=1),
            PointField(name='rgb', offset=12, datatype=PointField.UINT32, count=1)
        ]

        header = Header()
        header.stamp = self.get_timestamp()
        header.frame_id = frame_id

        pc2_msg = pc2.create_cloud(header, fields, points)
        self.depth_pc_pub.publish(pc2_msg)

    def is_pose_significantly_different(self, new_pose, prev_pose, trans_diff_threshold=0.05, rot_diff_threshold=5.0):
        """
        Check if the new pose is significantly different from the previous pose.
        Args:
            new_pose (np.ndarray): The new pose matrix.
            prev_pose (np.ndarray): The previous pose matrix.
        """
        # Check if the new pose is significantly different from the previous pose
        if prev_pose is None:
            return True
        
        # Calclate translation diffrence
        trans_diff = np.linalg.norm(new_pose[:3, 3] - prev_pose[:3, 3])
        # Calculate rotation difference
        r1 = R.from_matrix(prev_pose[:3, :3])
        r2 = R.from_matrix(new_pose[:3, :3])
        delta_r = r1.inv() * r2
        angle_deg = np.degrees(np.abs(delta_r.magnitude()))

        return trans_diff > trans_diff_threshold or angle_deg > rot_diff_threshold

    def timer_callback(self):    
        # Get all the images and pose
        pose = self.get_pose()
        rgb = self.get_rgb()
        depth = self.get_depth()

        # Check if any of the images ore pose are None
        if pose is None or rgb is None or depth is None:
            reason = (
                "pose" if pose is None else
                "RGB" if rgb is None else
                "depth"
            )
            self.get_logger().warn(f"Skipping frame: {reason} is None.")
            return

        # Check if the pose is significantly different from the previous pose
        min_trans = self.pose_min_translation
        min_rot = self.pose_min_rotation 
        if self.pose_buffer:
            last_pose_matrix = pose_msg_to_matrix(self.pose_buffer[-1])
            if not self.is_pose_significantly_different(new_pose=pose, prev_pose=last_pose_matrix, 
                                                        trans_diff_threshold=min_trans, rot_diff_threshold=min_rot):
                self.get_logger().info("Pose not significantly different. Skipping frame.")
                return
        
        # Convert the timestamps to seconds
        rgb_time = convert_stamp_to_sec(self.rgb_stamp)
        depth_time = convert_stamp_to_sec(self.depth_stamp)
        pose_time = convert_stamp_to_sec(self.pose_stamp)

        # Check if the timestamps are synchronized
        max_diff = max(abs(rgb_time - depth_time), abs(rgb_time - pose_time), abs(depth_time - pose_time))
        if max_diff > 0.01:
            self.get_logger().warn(f"Timestamps not synchronized (Δ={max_diff:.3f}s). Skipping frame.")
            return

        self.publish_pose_array()
        self.open_fusion_process(rgb, depth, pose)

        print("----------------------------------------------")
        # For debugging purposes
        self.publish_depth_pointcloud(rgb, depth, frame_id='camera')